{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susannatangg/aoc/blob/master/DoodleStories%20Eric%20Copy%203/30%208%3A42pm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create directory to store the dataset (optional)\n",
        "!mkdir -p datasets\n",
        "\n",
        "# Download the dataset\n",
        "!wget http://cvssp.org/data/fscoco/fscoco.tar.gz -P datasets/\n",
        "\n",
        "# Extract the tar.gz file\n",
        "!tar -xzf datasets/fscoco.tar.gz -C datasets/\n",
        "\n",
        "print(\"Dataset downloaded and extracted successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4r-oSY6c42q",
        "outputId": "f4c0e807-c0ee-440c-ebba-92c54339dbc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-31 01:35:10--  http://cvssp.org/data/fscoco/fscoco.tar.gz\n",
            "Resolving cvssp.org (cvssp.org)... 131.227.95.12\n",
            "Connecting to cvssp.org (cvssp.org)|131.227.95.12|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cvssp.org/data/fscoco/fscoco.tar.gz [following]\n",
            "--2025-03-31 01:35:10--  https://cvssp.org/data/fscoco/fscoco.tar.gz\n",
            "Connecting to cvssp.org (cvssp.org)|131.227.95.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2378884718 (2.2G) [application/x-gzip]\n",
            "Saving to: ‘datasets/fscoco.tar.gz’\n",
            "\n",
            "fscoco.tar.gz       100%[===================>]   2.21G  22.6MB/s    in 2m 9s   \n",
            "\n",
            "2025-03-31 01:37:20 (17.6 MB/s) - ‘datasets/fscoco.tar.gz’ saved [2378884718/2378884718]\n",
            "\n",
            "Dataset downloaded and extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset_path = \"datasets/fscoco/\"\n",
        "sketch_dir = os.path.join(dataset_path, \"sketchycoco\")  # Using sketchycoco instead of raster_sketches\n",
        "vector_dir = os.path.join(dataset_path, \"vector_sketches\")\n",
        "output_dir = os.path.join(dataset_path, \"processed_sketches\")\n",
        "output_dir_sub = os.path.join(dataset_path, \"processed_sub\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "BAVZeIVMe1xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sketch(img_path, size=(224, 224)):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read in grayscale\n",
        "    if img is None:\n",
        "        print(f\"Warning: Unable to read image {img_path}\")\n",
        "        return None  # Return None to prevent errors\n",
        "\n",
        "    img = cv2.resize(img, size)  # Resize to fixed dimensions\n",
        "    _, img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY)  # Binarize the image\n",
        "    img = img / 255.0  # Normalize pixel values\n",
        "    return img\n",
        "\n",
        "# Process sketches\n",
        "for root, _, files in os.walk(sketch_dir):\n",
        "    for file in files:\n",
        "        img_path = os.path.join(root, file)\n",
        "        processed_img = preprocess_sketch(img_path)\n",
        "\n",
        "        if processed_img is not None:\n",
        "            # Preserve relative subdirectory structure\n",
        "            relative_path = os.path.relpath(img_path, sketch_dir)\n",
        "            save_path = os.path.join(output_dir, relative_path)\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "\n",
        "            cv2.imwrite(save_path, (processed_img * 255).astype('uint8'))\n",
        "        else:\n",
        "            print(f\"Skipping file: {img_path}\")"
      ],
      "metadata": {
        "id": "3a9MfHH7e5r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_sketch(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        return None\n",
        "\n",
        "    # Flip image horizontally\n",
        "    flipped = cv2.flip(img, 1)\n",
        "\n",
        "    # Rotate image\n",
        "    (h, w) = img.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    rotation_matrix = cv2.getRotationMatrix2D(center, 15, 1.0)\n",
        "    rotated = cv2.warpAffine(img, rotation_matrix, (w, h))\n",
        "\n",
        "    return [img, flipped, rotated]\n",
        "\n",
        "# Apply augmentation\n",
        "for root, _, files in os.walk(sketch_dir):\n",
        "    for file in files:\n",
        "        img_path = os.path.join(sketch_dir, file)\n",
        "        augmented_images = augment_sketch(img_path)\n",
        "\n",
        "        if augmented_images:\n",
        "            for i, aug_img in enumerate(augmented_images):\n",
        "                aug_filename = f\"{os.path.splitext(file)[0]}_aug{i}.png\"\n",
        "                cv2.imwrite(os.path.join(output_dir, aug_filename), aug_img)\n"
      ],
      "metadata": {
        "id": "1vNRyw6GijBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "old_dir = 'datasets/fscoco/processed_sketches'\n",
        "new_dir = 'datasets/fscoco/processed_sub'\n",
        "\n",
        "# Renaming the directory\n",
        "os.rename(old_dir, new_dir)"
      ],
      "metadata": {
        "id": "E40dXZBL1QKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sketch_files = os.listdir(output_dir_sub)\n",
        "train_files, temp_files = train_test_split(sketch_files, test_size=0.2, random_state=42)\n",
        "val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "z2SgiTx9qUjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Move files to separate directories\n",
        "train_dir = \"datasets/fscoco/processed_sketches/train\"\n",
        "val_dir = \"datasets/fscoco/processed_sketches/val\"\n",
        "test_dir = \"datasets/fscoco/processed_sketches/test\"\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "def move_files_to_target(files, target_dir):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(output_dir_sub, file)\n",
        "        shutil.move(file_path, os.path.join(target_dir, file))\n",
        "\n",
        "move_files_to_target(train_files, train_dir)\n",
        "move_files_to_target(val_files, val_dir)\n",
        "move_files_to_target(test_files, test_dir)\n",
        "\n",
        "print(\"Train/Validation/Test split completed.\")\n"
      ],
      "metadata": {
        "id": "l2HGN062kp6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa5019b-55d5-4b16-b8a1-49868f959959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Validation/Test split completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move png images, get rid of all the subfolders\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "processed_sketches_dir = \"datasets/fscoco/processed_sketches\"\n",
        "\n",
        "dirs = ['train', 'val', 'test']\n",
        "\n",
        "for dir_name in dirs:\n",
        "    dir_path = os.path.join(processed_sketches_dir, dir_name)\n",
        "\n",
        "    if os.path.isdir(dir_path):\n",
        "        # Walk through the subdirectories and move the .png files\n",
        "        for root, dirs, files in os.walk(dir_path):\n",
        "            for file in files:\n",
        "                if file.endswith(\".png\"):\n",
        "                    old_path = os.path.join(root, file)\n",
        "                    new_path = os.path.join(dir_path, file)\n",
        "\n",
        "                    shutil.move(old_path, new_path)\n",
        "\n",
        "        for root, dirs, files in os.walk(dir_path, topdown=False):\n",
        "            for dir_name in dirs:\n",
        "                subfolder_path = os.path.join(root, dir_name)\n",
        "                if not os.listdir(subfolder_path):\n",
        "                    shutil.rmtree(subfolder_path)\n",
        "\n",
        "    else:\n",
        "        print(f\"Directory {dir_path} does not exist.\")\n",
        "\n",
        "print(\"All PNG files have been moved and subfolders removed.\")"
      ],
      "metadata": {
        "id": "O7YyGe3axR8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51950f4-8d73-4fce-94fc-b3711315aeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All PNG files have been moved and subfolders removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move txt files, get rid of all the subfolders\n",
        "\n",
        "text_dir = \"datasets/fscoco/text\"\n",
        "\n",
        "for subdir in os.listdir(text_dir):\n",
        "    subdir_path = os.path.join(text_dir, subdir)\n",
        "\n",
        "    if os.path.isdir(subdir_path):\n",
        "        for file in os.listdir(subdir_path):\n",
        "            file_path = os.path.join(subdir_path, file)\n",
        "\n",
        "            if file.endswith(\".txt\"):\n",
        "                new_path = os.path.join(text_dir, file)\n",
        "                if os.path.exists(new_path):\n",
        "                    print(f\"Skipping {file} - already exists in {text_dir}\")\n",
        "                else:\n",
        "                    shutil.move(file_path, new_path)\n",
        "\n",
        "        os.rmdir(subdir_path)\n",
        "        print(f\"Removed empty directory: {subdir_path}\")\n",
        "\n",
        "print(\"All txt files have been moved, and subfolders have been deleted.\")"
      ],
      "metadata": {
        "id": "RtD4ipb66A--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275f2ed4-5666-4bb7-caad-b7400f76abde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed empty directory: datasets/fscoco/text/73\n",
            "Removed empty directory: datasets/fscoco/text/19\n",
            "Removed empty directory: datasets/fscoco/text/77\n",
            "Removed empty directory: datasets/fscoco/text/41\n",
            "Removed empty directory: datasets/fscoco/text/16\n",
            "Removed empty directory: datasets/fscoco/text/9\n",
            "Removed empty directory: datasets/fscoco/text/60\n",
            "Removed empty directory: datasets/fscoco/text/92\n",
            "Removed empty directory: datasets/fscoco/text/24\n",
            "Removed empty directory: datasets/fscoco/text/96\n",
            "Removed empty directory: datasets/fscoco/text/89\n",
            "Removed empty directory: datasets/fscoco/text/2\n",
            "Removed empty directory: datasets/fscoco/text/94\n",
            "Removed empty directory: datasets/fscoco/text/74\n",
            "Removed empty directory: datasets/fscoco/text/10\n",
            "Removed empty directory: datasets/fscoco/text/79\n",
            "Removed empty directory: datasets/fscoco/text/86\n",
            "Removed empty directory: datasets/fscoco/text/85\n",
            "Removed empty directory: datasets/fscoco/text/54\n",
            "Removed empty directory: datasets/fscoco/text/39\n",
            "Removed empty directory: datasets/fscoco/text/57\n",
            "Removed empty directory: datasets/fscoco/text/46\n",
            "Removed empty directory: datasets/fscoco/text/97\n",
            "Removed empty directory: datasets/fscoco/text/5\n",
            "Removed empty directory: datasets/fscoco/text/11\n",
            "Removed empty directory: datasets/fscoco/text/62\n",
            "Removed empty directory: datasets/fscoco/text/37\n",
            "Removed empty directory: datasets/fscoco/text/8\n",
            "Removed empty directory: datasets/fscoco/text/95\n",
            "Removed empty directory: datasets/fscoco/text/14\n",
            "Removed empty directory: datasets/fscoco/text/80\n",
            "Removed empty directory: datasets/fscoco/text/82\n",
            "Removed empty directory: datasets/fscoco/text/7\n",
            "Removed empty directory: datasets/fscoco/text/69\n",
            "Removed empty directory: datasets/fscoco/text/18\n",
            "Removed empty directory: datasets/fscoco/text/34\n",
            "Removed empty directory: datasets/fscoco/text/3\n",
            "Removed empty directory: datasets/fscoco/text/13\n",
            "Removed empty directory: datasets/fscoco/text/91\n",
            "Removed empty directory: datasets/fscoco/text/52\n",
            "Removed empty directory: datasets/fscoco/text/21\n",
            "Removed empty directory: datasets/fscoco/text/20\n",
            "Removed empty directory: datasets/fscoco/text/66\n",
            "Removed empty directory: datasets/fscoco/text/32\n",
            "Removed empty directory: datasets/fscoco/text/25\n",
            "Removed empty directory: datasets/fscoco/text/42\n",
            "Removed empty directory: datasets/fscoco/text/44\n",
            "Removed empty directory: datasets/fscoco/text/100\n",
            "Removed empty directory: datasets/fscoco/text/56\n",
            "Removed empty directory: datasets/fscoco/text/33\n",
            "Removed empty directory: datasets/fscoco/text/4\n",
            "Removed empty directory: datasets/fscoco/text/51\n",
            "Removed empty directory: datasets/fscoco/text/23\n",
            "Removed empty directory: datasets/fscoco/text/43\n",
            "Removed empty directory: datasets/fscoco/text/81\n",
            "Removed empty directory: datasets/fscoco/text/48\n",
            "Removed empty directory: datasets/fscoco/text/36\n",
            "Removed empty directory: datasets/fscoco/text/1\n",
            "Removed empty directory: datasets/fscoco/text/31\n",
            "Removed empty directory: datasets/fscoco/text/72\n",
            "Removed empty directory: datasets/fscoco/text/55\n",
            "Removed empty directory: datasets/fscoco/text/83\n",
            "Removed empty directory: datasets/fscoco/text/30\n",
            "Removed empty directory: datasets/fscoco/text/26\n",
            "Removed empty directory: datasets/fscoco/text/27\n",
            "Removed empty directory: datasets/fscoco/text/64\n",
            "Removed empty directory: datasets/fscoco/text/49\n",
            "Removed empty directory: datasets/fscoco/text/59\n",
            "Removed empty directory: datasets/fscoco/text/67\n",
            "Removed empty directory: datasets/fscoco/text/35\n",
            "Removed empty directory: datasets/fscoco/text/75\n",
            "Removed empty directory: datasets/fscoco/text/93\n",
            "Removed empty directory: datasets/fscoco/text/90\n",
            "Removed empty directory: datasets/fscoco/text/50\n",
            "Removed empty directory: datasets/fscoco/text/63\n",
            "Removed empty directory: datasets/fscoco/text/88\n",
            "Removed empty directory: datasets/fscoco/text/45\n",
            "Removed empty directory: datasets/fscoco/text/22\n",
            "Removed empty directory: datasets/fscoco/text/61\n",
            "Removed empty directory: datasets/fscoco/text/6\n",
            "Removed empty directory: datasets/fscoco/text/78\n",
            "Removed empty directory: datasets/fscoco/text/15\n",
            "Removed empty directory: datasets/fscoco/text/40\n",
            "Removed empty directory: datasets/fscoco/text/87\n",
            "Removed empty directory: datasets/fscoco/text/84\n",
            "Removed empty directory: datasets/fscoco/text/12\n",
            "Removed empty directory: datasets/fscoco/text/65\n",
            "Removed empty directory: datasets/fscoco/text/70\n",
            "Removed empty directory: datasets/fscoco/text/98\n",
            "Removed empty directory: datasets/fscoco/text/68\n",
            "Removed empty directory: datasets/fscoco/text/47\n",
            "Removed empty directory: datasets/fscoco/text/71\n",
            "Removed empty directory: datasets/fscoco/text/38\n",
            "Removed empty directory: datasets/fscoco/text/76\n",
            "Removed empty directory: datasets/fscoco/text/28\n",
            "Removed empty directory: datasets/fscoco/text/29\n",
            "Removed empty directory: datasets/fscoco/text/58\n",
            "Removed empty directory: datasets/fscoco/text/53\n",
            "Removed empty directory: datasets/fscoco/text/99\n",
            "Removed empty directory: datasets/fscoco/text/17\n",
            "All txt files have been moved, and subfolders have been deleted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def build_vocab(caption_dir):\n",
        "    vocab_counter = Counter()\n",
        "\n",
        "    # Read all captions\n",
        "    for caption_file in os.listdir(caption_dir):\n",
        "        caption_path = os.path.join(caption_dir, caption_file)\n",
        "        if caption_file.endswith('.txt'):\n",
        "            with open(caption_path, 'r') as file:\n",
        "                caption = file.read().strip().lower()\n",
        "                vocab_counter.update(caption.split())\n",
        "\n",
        "    # build vocab mapping\n",
        "    vocab = {word: idx + 1 for idx, (word, _) in enumerate(vocab_counter.most_common())}\n",
        "    vocab[\"<PAD>\"] = 0\n",
        "    vocab[\"<UNK>\"] = len(vocab)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "caption_dir = \"datasets/fscoco/text\"\n",
        "vocab = build_vocab(caption_dir)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n"
      ],
      "metadata": {
        "id": "dfZb0PsT7he8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f67813-1945-4599-8da0-53dc7cbe8f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 2838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. CUDA Initialization Safety\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Always enable for precise error location\n",
        "\n",
        "def train_doodle_model(train_dir, val_dir, test_dir, caption_dir, vocab, num_epochs=5):\n",
        "    print(\"Initializing training with CUDA safety checks\")\n",
        "\n",
        "    # 2. Device Configuration\n",
        "    device = torch.device(\"cpu\")\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            # Test CUDA with a simple operation before committing\n",
        "            _ = torch.cuda.FloatTensor(1).zero_()\n",
        "            device = torch.device(\"cuda\")\n",
        "            print(\"CUDA initialized successfully\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"CUDA failed: {e}. Permanent fallback to CPU\")\n",
        "            torch.cuda.is_available = lambda: False  # Disable future CUDA checks\n",
        "\n",
        "    # 3. Vocabulary Finalization\n",
        "    VOCAB_SIZE = len(vocab) + 4  # Original vocab + special tokens\n",
        "    print(f\"Final vocabulary size: {VOCAB_SIZE}\")\n",
        "\n",
        "    # 4. Data Pipeline with Double Safety\n",
        "    class SafeDoodleDataset(DoodleDataset):\n",
        "        def __getitem__(self, idx):\n",
        "            try:\n",
        "                img, caption = super().__getitem__(idx)\n",
        "                # Double index validation\n",
        "                caption = torch.clamp(caption, 0, VOCAB_SIZE-1)\n",
        "                return img, caption\n",
        "            except Exception as e:\n",
        "                print(f\"Error in sample {idx}: {e}\")\n",
        "                # Return valid dummy data\n",
        "                return torch.rand(3, 224, 224), torch.tensor([1, 2], dtype=torch.long)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(3),\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 5. Model Architecture Verification\n",
        "    class DoodleStoryModel(nn.Module):\n",
        "        def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=2):\n",
        "            super().__init__()\n",
        "            # Architecture must match exactly with VOCAB_SIZE\n",
        "            self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "            self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_size, vocab_size)  # Critical: output size matches vocab\n",
        "\n",
        "        def forward(self, images, captions):\n",
        "            # Your existing forward logic\n",
        "            embedded = self.embed(captions)\n",
        "            output, _ = self.lstm(embedded)\n",
        "            return self.fc(output)\n",
        "\n",
        "    # 6. Data Loading with Enhanced Validation\n",
        "    train_dataset = SafeDoodleDataset(train_dir, caption_dir, transform,\n",
        "                                     lambda x: x.split(), vocab)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,\n",
        "                             collate_fn=collate_fn, pin_memory=False)\n",
        "\n",
        "    # 7. Model Initialization\n",
        "    model = DoodleStoryModel(vocab_size=VOCAB_SIZE).to(device)\n",
        "    print(f\"Model structure verified - Output size: {VOCAB_SIZE}\")\n",
        "\n",
        "    # 8. Training Loop with CUDA Isolation\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    try:\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch_idx, (images, captions) in enumerate(train_loader):\n",
        "                # Final index sanitization\n",
        "                captions = torch.clamp(captions, 0, VOCAB_SIZE-1)\n",
        "\n",
        "                # Device transfer with validation\n",
        "                images = images.to(device, non_blocking=False)\n",
        "                captions = captions.to(device, non_blocking=False)\n",
        "\n",
        "                # Forward pass with dimension verification\n",
        "                outputs = model(images, captions[:, :-1])\n",
        "                assert outputs.shape[-1] == VOCAB_SIZE, \"Model output mismatch!\"\n",
        "\n",
        "                # Loss calculation\n",
        "                loss = criterion(outputs.view(-1, VOCAB_SIZE),\n",
        "                              captions[:, 1:].contiguous().view(-1))\n",
        "\n",
        "                # Backpropagation\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "                optimizer.step()\n",
        "\n",
        "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Critical error: {e}\")\n",
        "        if 'CUDA' in str(e):\n",
        "            print(\"Executing emergency CPU fallback\")\n",
        "            device = torch.device(\"cpu\")\n",
        "            model = model.to(device)\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "            # Continue training on CPU if possible\n",
        "\n",
        "    return model, vocab"
      ],
      "metadata": {
        "id": "RodnzLfVq-71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute training\n",
        "model, vocab = train_doodle_model(train_dir, val_dir, test_dir, caption_dir, vocab)"
      ],
      "metadata": {
        "id": "zTSDwvDG63aT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1db364-de04-4eb5-82f8-06fc66eb5f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing training with CUDA safety checks\n",
            "CUDA initialized successfully\n",
            "Final vocabulary size: 2842\n",
            "Model structure verified - Output size: 2842\n",
            "Epoch 1, Batch 0, Loss: 7.9565\n",
            "Epoch 1, Batch 1, Loss: 7.9329\n",
            "Epoch 1, Batch 2, Loss: 7.8957\n",
            "Epoch 1, Batch 3, Loss: 7.8528\n",
            "Epoch 1, Batch 4, Loss: 7.7320\n",
            "Epoch 1, Batch 5, Loss: 7.5625\n",
            "Epoch 1, Batch 6, Loss: 7.2219\n",
            "Epoch 1, Batch 7, Loss: 6.8427\n",
            "Epoch 1, Batch 8, Loss: 6.7060\n",
            "Epoch 1, Batch 9, Loss: 6.6099\n",
            "Epoch 1, Batch 10, Loss: 6.4728\n",
            "Epoch 1, Batch 11, Loss: 6.2870\n",
            "Epoch 1, Batch 12, Loss: 6.5887\n",
            "Epoch 1, Batch 13, Loss: 5.7408\n",
            "Epoch 1, Batch 14, Loss: 5.8980\n",
            "Epoch 1, Batch 15, Loss: 5.7426\n",
            "Epoch 1, Batch 16, Loss: 5.9880\n",
            "Epoch 1, Batch 17, Loss: 5.7926\n",
            "Epoch 1, Batch 18, Loss: 5.6713\n",
            "Epoch 1, Batch 19, Loss: 5.8017\n",
            "Epoch 1, Batch 20, Loss: 5.5397\n",
            "Epoch 1, Batch 21, Loss: 5.8246\n",
            "Epoch 1, Batch 22, Loss: 5.3548\n",
            "Epoch 1, Batch 23, Loss: 5.8261\n",
            "Epoch 1, Batch 24, Loss: 5.4494\n",
            "Epoch 1, Batch 25, Loss: 5.7166\n",
            "Epoch 1, Batch 26, Loss: 5.3819\n",
            "Epoch 1, Batch 27, Loss: 5.7195\n",
            "Epoch 1, Batch 28, Loss: 5.6360\n",
            "Epoch 1, Batch 29, Loss: 5.4703\n",
            "Epoch 1, Batch 30, Loss: 5.4627\n",
            "Epoch 1, Batch 31, Loss: 5.1160\n",
            "Epoch 1, Batch 32, Loss: 5.6043\n",
            "Epoch 1, Batch 33, Loss: 5.5452\n",
            "Epoch 1, Batch 34, Loss: 5.4757\n",
            "Epoch 1, Batch 35, Loss: 5.0251\n",
            "Epoch 1, Batch 36, Loss: 5.0423\n",
            "Epoch 1, Batch 37, Loss: 5.2701\n",
            "Epoch 1, Batch 38, Loss: 5.0366\n",
            "Epoch 1, Batch 39, Loss: 4.9143\n",
            "Epoch 1, Batch 40, Loss: 4.8732\n",
            "Epoch 1, Batch 41, Loss: 4.9523\n",
            "Epoch 1, Batch 42, Loss: 5.1849\n",
            "Epoch 1, Batch 43, Loss: 5.1309\n",
            "Epoch 1, Batch 44, Loss: 5.2337\n",
            "Epoch 1, Batch 45, Loss: 4.8919\n",
            "Epoch 1, Batch 46, Loss: 5.4193\n",
            "Epoch 1, Batch 47, Loss: 5.3377\n",
            "Epoch 1, Batch 48, Loss: 4.8207\n",
            "Epoch 1, Batch 49, Loss: 5.3225\n",
            "Epoch 1, Batch 50, Loss: 5.2333\n",
            "Epoch 1, Batch 51, Loss: 5.2575\n",
            "Epoch 1, Batch 52, Loss: 4.9829\n",
            "Epoch 1, Batch 53, Loss: 4.8656\n",
            "Epoch 1, Batch 54, Loss: 4.9020\n",
            "Epoch 1, Batch 55, Loss: 4.8525\n",
            "Epoch 1, Batch 56, Loss: 4.6677\n",
            "Epoch 1, Batch 57, Loss: 4.5738\n",
            "Epoch 1, Batch 58, Loss: 4.6296\n",
            "Epoch 1, Batch 59, Loss: 4.5833\n",
            "Epoch 1, Batch 60, Loss: 5.1757\n",
            "Epoch 1, Batch 61, Loss: 4.9829\n",
            "Epoch 1, Batch 62, Loss: 4.4710\n",
            "Epoch 1, Batch 63, Loss: 4.9551\n",
            "Epoch 1, Batch 64, Loss: 5.1443\n",
            "Epoch 1, Batch 65, Loss: 4.5549\n",
            "Epoch 1, Batch 66, Loss: 4.6557\n",
            "Epoch 1, Batch 67, Loss: 4.8573\n",
            "Epoch 1, Batch 68, Loss: 4.6279\n",
            "Epoch 1, Batch 69, Loss: 4.9486\n",
            "Epoch 1, Batch 70, Loss: 4.7701\n",
            "Epoch 1, Batch 71, Loss: 4.7955\n",
            "Epoch 1, Batch 72, Loss: 4.7010\n",
            "Epoch 1, Batch 73, Loss: 5.0052\n",
            "Epoch 1, Batch 74, Loss: 4.4066\n",
            "Epoch 1, Batch 75, Loss: 4.6842\n",
            "Epoch 1, Batch 76, Loss: 5.1662\n",
            "Epoch 1, Batch 77, Loss: 4.8203\n",
            "Epoch 1, Batch 78, Loss: 5.0784\n",
            "Epoch 1, Batch 79, Loss: 4.6960\n",
            "Epoch 1, Batch 80, Loss: 4.6372\n",
            "Epoch 1, Batch 81, Loss: 4.9929\n",
            "Epoch 1, Batch 82, Loss: 4.6769\n",
            "Epoch 1, Batch 83, Loss: 4.4373\n",
            "Epoch 1, Batch 84, Loss: 5.4162\n",
            "Epoch 1, Batch 85, Loss: 4.4391\n",
            "Epoch 1, Batch 86, Loss: 4.5733\n",
            "Epoch 1, Batch 87, Loss: 4.5677\n",
            "Epoch 1, Batch 88, Loss: 4.2517\n",
            "Epoch 1, Batch 89, Loss: 4.6198\n",
            "Epoch 1, Batch 90, Loss: 5.3853\n",
            "Epoch 1, Batch 91, Loss: 4.9594\n",
            "Epoch 1, Batch 92, Loss: 4.3277\n",
            "Epoch 1, Batch 93, Loss: 4.2776\n",
            "Epoch 1, Batch 94, Loss: 5.0110\n",
            "Epoch 1, Batch 95, Loss: 4.6457\n",
            "Epoch 1, Batch 96, Loss: 4.9198\n",
            "Epoch 1, Batch 97, Loss: 5.0419\n",
            "Epoch 1, Batch 98, Loss: 4.4958\n",
            "Epoch 1, Batch 99, Loss: 4.5618\n",
            "Epoch 1, Batch 100, Loss: 4.9621\n",
            "Epoch 1, Batch 101, Loss: 4.5812\n",
            "Epoch 1, Batch 102, Loss: 4.9718\n",
            "Epoch 1, Batch 103, Loss: 4.7155\n",
            "Epoch 1, Batch 104, Loss: 4.4435\n",
            "Epoch 1, Batch 105, Loss: 4.2793\n",
            "Epoch 1, Batch 106, Loss: 4.4433\n",
            "Epoch 1, Batch 107, Loss: 4.4524\n",
            "Epoch 1, Batch 108, Loss: 4.5532\n",
            "Epoch 1, Batch 109, Loss: 4.4967\n",
            "Epoch 1, Batch 110, Loss: 4.9064\n",
            "Epoch 1, Batch 111, Loss: 4.7517\n",
            "Epoch 1, Batch 112, Loss: 4.4741\n",
            "Epoch 1, Batch 113, Loss: 4.1118\n",
            "Epoch 1, Batch 114, Loss: 4.1805\n",
            "Epoch 1, Batch 115, Loss: 4.6903\n",
            "Epoch 1, Batch 116, Loss: 4.2522\n",
            "Epoch 1, Batch 117, Loss: 4.5767\n",
            "Epoch 1, Batch 118, Loss: 4.3707\n",
            "Epoch 1, Batch 119, Loss: 4.4272\n",
            "Epoch 1, Batch 120, Loss: 4.8119\n",
            "Epoch 1, Batch 121, Loss: 4.3685\n",
            "Epoch 1, Batch 122, Loss: 4.3416\n",
            "Epoch 1, Batch 123, Loss: 3.8812\n",
            "Epoch 1, Batch 124, Loss: 4.5440\n",
            "Epoch 1, Batch 125, Loss: 4.1634\n",
            "Epoch 1, Batch 126, Loss: 4.3654\n",
            "Epoch 1, Batch 127, Loss: 4.4463\n",
            "Epoch 1, Batch 128, Loss: 4.1792\n",
            "Epoch 1, Batch 129, Loss: 4.4874\n",
            "Epoch 1, Batch 130, Loss: 4.4664\n",
            "Epoch 1, Batch 131, Loss: 4.1910\n",
            "Epoch 1, Batch 132, Loss: 4.1126\n",
            "Epoch 1, Batch 133, Loss: 4.2138\n",
            "Epoch 1, Batch 134, Loss: 4.3704\n",
            "Epoch 1, Batch 135, Loss: 4.3814\n",
            "Epoch 1, Batch 136, Loss: 3.9340\n",
            "Epoch 1, Batch 137, Loss: 3.6569\n",
            "Epoch 1, Batch 138, Loss: 4.3518\n",
            "Epoch 1, Batch 139, Loss: 4.5865\n",
            "Epoch 1, Batch 140, Loss: 3.8012\n",
            "Epoch 1, Batch 141, Loss: 4.5680\n",
            "Epoch 1, Batch 142, Loss: 4.3445\n",
            "Epoch 1, Batch 143, Loss: 4.2482\n",
            "Epoch 1, Batch 144, Loss: 4.3650\n",
            "Epoch 1, Batch 145, Loss: 3.8326\n",
            "Epoch 1, Batch 146, Loss: 4.1167\n",
            "Epoch 1, Batch 147, Loss: 4.3385\n",
            "Epoch 1, Batch 148, Loss: 4.0441\n",
            "Epoch 1, Batch 149, Loss: 4.4473\n",
            "Epoch 1, Batch 150, Loss: 4.5582\n",
            "Epoch 1, Batch 151, Loss: 3.9506\n",
            "Epoch 1, Batch 152, Loss: 3.9649\n",
            "Epoch 1, Batch 153, Loss: 4.2016\n",
            "Epoch 1, Batch 154, Loss: 3.8113\n",
            "Epoch 1, Batch 155, Loss: 4.1054\n",
            "Epoch 1, Batch 156, Loss: 4.7558\n",
            "Epoch 1, Batch 157, Loss: 3.9537\n",
            "Epoch 1, Batch 158, Loss: 4.3920\n",
            "Epoch 1, Batch 159, Loss: 4.3468\n",
            "Epoch 1, Batch 160, Loss: 3.7655\n",
            "Epoch 1, Batch 161, Loss: 4.0671\n",
            "Epoch 1, Batch 162, Loss: 4.3803\n",
            "Epoch 1, Batch 163, Loss: 3.9226\n",
            "Epoch 1, Batch 164, Loss: 4.1792\n",
            "Epoch 1, Batch 165, Loss: 3.9973\n",
            "Epoch 1, Batch 166, Loss: 4.4946\n",
            "Epoch 1, Batch 167, Loss: 4.3064\n",
            "Epoch 1, Batch 168, Loss: 4.2611\n",
            "Epoch 1, Batch 169, Loss: 3.5250\n",
            "Epoch 1, Batch 170, Loss: 4.1580\n",
            "Epoch 1, Batch 171, Loss: 4.0270\n",
            "Epoch 1, Batch 172, Loss: 4.2577\n",
            "Epoch 1, Batch 173, Loss: 3.8106\n",
            "Epoch 1, Batch 174, Loss: 3.6663\n",
            "Epoch 1, Batch 175, Loss: 4.3613\n",
            "Epoch 1, Batch 176, Loss: 4.8220\n",
            "Epoch 1, Batch 177, Loss: 3.8220\n",
            "Epoch 1, Batch 178, Loss: 3.9173\n",
            "Epoch 1, Batch 179, Loss: 4.3150\n",
            "Epoch 1, Batch 180, Loss: 4.5108\n",
            "Epoch 1, Batch 181, Loss: 3.9374\n",
            "Epoch 1, Batch 182, Loss: 4.5803\n",
            "Epoch 1, Batch 183, Loss: 4.0206\n",
            "Epoch 1, Batch 184, Loss: 3.7925\n",
            "Epoch 1, Batch 185, Loss: 3.5684\n",
            "Epoch 1, Batch 186, Loss: 3.9401\n",
            "Epoch 1, Batch 187, Loss: 4.0511\n",
            "Epoch 1, Batch 188, Loss: 3.5790\n",
            "Epoch 1, Batch 189, Loss: 4.0767\n",
            "Epoch 1, Batch 190, Loss: 4.0490\n",
            "Epoch 1, Batch 191, Loss: 3.5945\n",
            "Epoch 1, Batch 192, Loss: 4.8883\n",
            "Epoch 1, Batch 193, Loss: 4.0942\n",
            "Epoch 1, Batch 194, Loss: 4.2105\n",
            "Epoch 1, Batch 195, Loss: 3.7271\n",
            "Epoch 1, Batch 196, Loss: 4.3291\n",
            "Epoch 1, Batch 197, Loss: 3.9294\n",
            "Epoch 1, Batch 198, Loss: 4.0208\n",
            "Epoch 1, Batch 199, Loss: 3.6537\n",
            "Epoch 1, Batch 200, Loss: 3.9029\n",
            "Epoch 1, Batch 201, Loss: 4.0299\n",
            "Epoch 1, Batch 202, Loss: 3.8187\n",
            "Epoch 1, Batch 203, Loss: 4.1090\n",
            "Epoch 1, Batch 204, Loss: 3.3400\n",
            "Epoch 1, Batch 205, Loss: 4.2090\n",
            "Epoch 1, Batch 206, Loss: 3.9059\n",
            "Epoch 1, Batch 207, Loss: 3.5986\n",
            "Epoch 1, Batch 208, Loss: 4.1174\n",
            "Epoch 1, Batch 209, Loss: 3.7281\n",
            "Epoch 1, Batch 210, Loss: 3.9267\n",
            "Epoch 1, Batch 211, Loss: 4.0904\n",
            "Epoch 1, Batch 212, Loss: 3.5997\n",
            "Epoch 1, Batch 213, Loss: 3.5514\n",
            "Epoch 1, Batch 214, Loss: 3.9275\n",
            "Epoch 1, Batch 215, Loss: 4.0221\n",
            "Epoch 1, Batch 216, Loss: 3.6129\n",
            "Epoch 1, Batch 217, Loss: 4.4483\n",
            "Epoch 1, Batch 218, Loss: 3.8543\n",
            "Epoch 1, Batch 219, Loss: 3.7828\n",
            "Epoch 1, Batch 220, Loss: 4.1701\n",
            "Epoch 1, Batch 221, Loss: 3.7272\n",
            "Epoch 1, Batch 222, Loss: 4.1257\n",
            "Epoch 1, Batch 223, Loss: 3.9400\n",
            "Epoch 1, Batch 224, Loss: 3.5657\n",
            "Epoch 1, Batch 225, Loss: 3.6375\n",
            "Epoch 1, Batch 226, Loss: 3.7128\n",
            "Epoch 1, Batch 227, Loss: 3.9731\n",
            "Epoch 1, Batch 228, Loss: 3.9531\n",
            "Epoch 1, Batch 229, Loss: 3.4948\n",
            "Epoch 1, Batch 230, Loss: 3.8364\n",
            "Epoch 1, Batch 231, Loss: 3.5161\n",
            "Epoch 1, Batch 232, Loss: 3.6665\n",
            "Epoch 1, Batch 233, Loss: 3.5142\n",
            "Epoch 1, Batch 234, Loss: 3.9537\n",
            "Epoch 1, Batch 235, Loss: 3.6458\n",
            "Epoch 1, Batch 236, Loss: 3.5752\n",
            "Epoch 1, Batch 237, Loss: 3.9847\n",
            "Epoch 1, Batch 238, Loss: 3.9362\n",
            "Epoch 1, Batch 239, Loss: 3.9701\n",
            "Epoch 1, Batch 240, Loss: 3.4439\n",
            "Epoch 1, Batch 241, Loss: 3.7778\n",
            "Epoch 1, Batch 242, Loss: 4.0474\n",
            "Epoch 1, Batch 243, Loss: 3.7641\n",
            "Epoch 1, Batch 244, Loss: 3.5947\n",
            "Epoch 1, Batch 245, Loss: 3.7790\n",
            "Epoch 1, Batch 246, Loss: 3.7184\n",
            "Epoch 1, Batch 247, Loss: 4.0151\n",
            "Epoch 1, Batch 248, Loss: 3.9335\n",
            "Epoch 1, Batch 249, Loss: 3.6148\n",
            "Epoch 2, Batch 0, Loss: 3.8728\n",
            "Epoch 2, Batch 1, Loss: 3.6920\n",
            "Epoch 2, Batch 2, Loss: 3.4370\n",
            "Epoch 2, Batch 3, Loss: 4.0043\n",
            "Epoch 2, Batch 4, Loss: 3.8505\n",
            "Epoch 2, Batch 5, Loss: 3.4353\n",
            "Epoch 2, Batch 6, Loss: 3.4519\n",
            "Epoch 2, Batch 7, Loss: 3.4281\n",
            "Epoch 2, Batch 8, Loss: 3.4613\n",
            "Epoch 2, Batch 9, Loss: 3.4639\n",
            "Epoch 2, Batch 10, Loss: 3.8043\n",
            "Epoch 2, Batch 11, Loss: 3.6161\n",
            "Epoch 2, Batch 12, Loss: 3.5151\n",
            "Epoch 2, Batch 13, Loss: 3.7777\n",
            "Epoch 2, Batch 14, Loss: 3.1656\n",
            "Epoch 2, Batch 15, Loss: 3.6513\n",
            "Epoch 2, Batch 16, Loss: 3.6844\n",
            "Epoch 2, Batch 17, Loss: 3.6170\n",
            "Epoch 2, Batch 18, Loss: 3.6550\n",
            "Epoch 2, Batch 19, Loss: 3.5548\n",
            "Epoch 2, Batch 20, Loss: 3.9640\n",
            "Epoch 2, Batch 21, Loss: 3.3346\n",
            "Epoch 2, Batch 22, Loss: 3.6136\n",
            "Epoch 2, Batch 23, Loss: 3.1239\n",
            "Epoch 2, Batch 24, Loss: 3.5649\n",
            "Epoch 2, Batch 25, Loss: 3.1392\n",
            "Epoch 2, Batch 26, Loss: 3.4141\n",
            "Epoch 2, Batch 27, Loss: 3.4334\n",
            "Epoch 2, Batch 28, Loss: 3.2708\n",
            "Epoch 2, Batch 29, Loss: 3.7145\n",
            "Epoch 2, Batch 30, Loss: 3.6879\n",
            "Epoch 2, Batch 31, Loss: 3.3321\n",
            "Epoch 2, Batch 32, Loss: 3.1607\n",
            "Epoch 2, Batch 33, Loss: 3.2644\n",
            "Epoch 2, Batch 34, Loss: 3.6801\n",
            "Epoch 2, Batch 35, Loss: 3.7617\n",
            "Epoch 2, Batch 36, Loss: 3.7464\n",
            "Epoch 2, Batch 37, Loss: 3.8615\n",
            "Epoch 2, Batch 38, Loss: 3.4293\n",
            "Epoch 2, Batch 39, Loss: 3.6168\n",
            "Epoch 2, Batch 40, Loss: 3.6828\n",
            "Epoch 2, Batch 41, Loss: 4.2050\n",
            "Epoch 2, Batch 42, Loss: 3.3810\n",
            "Epoch 2, Batch 43, Loss: 3.5835\n",
            "Epoch 2, Batch 44, Loss: 3.8139\n",
            "Epoch 2, Batch 45, Loss: 4.0308\n",
            "Epoch 2, Batch 46, Loss: 3.1479\n",
            "Epoch 2, Batch 47, Loss: 3.7139\n",
            "Epoch 2, Batch 48, Loss: 3.5561\n",
            "Epoch 2, Batch 49, Loss: 3.4709\n",
            "Epoch 2, Batch 50, Loss: 3.7598\n",
            "Epoch 2, Batch 51, Loss: 3.2240\n",
            "Epoch 2, Batch 52, Loss: 3.8974\n",
            "Epoch 2, Batch 53, Loss: 3.6045\n",
            "Epoch 2, Batch 54, Loss: 3.2051\n",
            "Epoch 2, Batch 55, Loss: 3.4810\n",
            "Epoch 2, Batch 56, Loss: 3.3706\n",
            "Epoch 2, Batch 57, Loss: 4.0269\n",
            "Epoch 2, Batch 58, Loss: 3.5050\n",
            "Epoch 2, Batch 59, Loss: 3.5931\n",
            "Epoch 2, Batch 60, Loss: 3.5825\n",
            "Epoch 2, Batch 61, Loss: 3.5636\n",
            "Epoch 2, Batch 62, Loss: 3.5053\n",
            "Epoch 2, Batch 63, Loss: 3.6882\n",
            "Epoch 2, Batch 64, Loss: 3.3198\n",
            "Epoch 2, Batch 65, Loss: 3.1952\n",
            "Epoch 2, Batch 66, Loss: 3.4554\n",
            "Epoch 2, Batch 67, Loss: 3.7219\n",
            "Epoch 2, Batch 68, Loss: 3.3457\n"
          ]
        }
      ]
    }
  ]
}